{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11835572,"sourceType":"datasetVersion","datasetId":7435749},{"sourceType":"datasetVersion","sourceId":12649139,"datasetId":7993694}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-08-02T07:17:34.047002Z","iopub.execute_input":"2025-08-02T07:17:34.047353Z","iopub.status.idle":"2025-08-02T07:17:34.054163Z","shell.execute_reply.started":"2025-08-02T07:17:34.047331Z","shell.execute_reply":"2025-08-02T07:17:34.053470Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/sbitraining/HACKATHON_TRAINING_DATA.CSV\n/kaggle/input/prediction/HACKATHON_PREDICTION_DATA.CSV.xls\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/sbitraining/HACKATHON_TRAINING_DATA.CSV')\nprint(df.head())\nprint(df.columns)","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:17:34.060706Z","iopub.execute_input":"2025-08-02T07:17:34.061472Z","iopub.status.idle":"2025-08-02T07:17:39.032574Z","shell.execute_reply.started":"2025-08-02T07:17:34.061447Z","shell.execute_reply":"2025-08-02T07:17:39.031556Z"},"trusted":true},"outputs":[{"name":"stdout","text":"   ACCT_AGE      LIMIT        OUTS  ACCT_RESIDUAL_TENURE  LOAN_TENURE  \\\n0     1.613  1005500.0   494161.89                 0.890          914   \n1     1.783  1005500.0   428072.24                 0.720          914   \n2     1.698  1005500.0   461364.10                 0.805          914   \n3     9.127  1005500.0  1204287.25                17.878         9862   \n4     9.296  1005500.0  1203224.25                17.708         9862   \n\n   INSTALAMT SI_FLG     AGE  VINTAGE  KYC_SCR  ... CREDIT_HISTORY_LENGTH1  \\\n0    38513.0      Y  57.663   18.601    110.0  ...              7yrs 6mon   \n1    38513.0      Y  57.833   18.771    110.0  ...              7yrs 6mon   \n2    38513.0      Y  57.748   18.686    110.0  ...              7yrs 6mon   \n3    12736.0      Y  52.302   14.039    110.0  ...             10yrs 8mon   \n4    12736.0      Y  52.472   14.209    110.0  ...             10yrs 8mon   \n\n  NO_OF_INQUIRIES1 INCOME_BAND1           AGREG_GROUP   PRODUCT_TYPE  \\\n0              0.0            G  #Total Xpress Credit  PERSONAL LOAN   \n1              0.0            G  #Total Xpress Credit  PERSONAL LOAN   \n2              0.0            G  #Total Xpress Credit  PERSONAL LOAN   \n3              1.0            D         #Housing Loan      HOME LOAN   \n4              1.0            D         #Housing Loan      HOME LOAN   \n\n   LATEST_CR_DAYS  LATEST_DR_DAYS  TIME_PERIOD  TARGET  UNIQUE_ID  \n0            60.0           45625        NOV24       0       2032  \n1            28.0           45687        JAN25       0       2033  \n2            28.0           45656        DEC24       0       2034  \n3             3.0           45625        NOV24       0       2035  \n4             2.0           45687        JAN25       0       2036  \n\n[5 rows x 139 columns]\nIndex(['ACCT_AGE', 'LIMIT', 'OUTS', 'ACCT_RESIDUAL_TENURE', 'LOAN_TENURE',\n       'INSTALAMT', 'SI_FLG', 'AGE', 'VINTAGE', 'KYC_SCR',\n       ...\n       'CREDIT_HISTORY_LENGTH1', 'NO_OF_INQUIRIES1', 'INCOME_BAND1',\n       'AGREG_GROUP', 'PRODUCT_TYPE', 'LATEST_CR_DAYS', 'LATEST_DR_DAYS',\n       'TIME_PERIOD', 'TARGET', 'UNIQUE_ID'],\n      dtype='object', length=139)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score\nfrom sklearn.utils import class_weight\nimport xgboost as xgb\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.metrics import Recall\n\n# ✅ Step 1: Preprocessing\nX = df.drop(columns=['TARGET', 'UNIQUE_ID']).copy()\ny = df['TARGET']\n\n# Handle placeholders and encode categoricals\nX.replace([\"\\\\N\", \"NA\", \"NaN\", \"null\", \"\"], np.nan, inplace=True)\nfor col in X.select_dtypes(include=['object', 'category']).columns:\n    X[col] = X[col].astype(str)\n    X[col] = LabelEncoder().fit_transform(X[col])\n\n# Impute missing values\nX = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(X), columns=X.columns)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.3, random_state=42)\nprint(f\"🔹 Train Rows: {len(X_train)}\")\nprint(f\"🔹 Test Rows:  {len(X_test)}\")","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:17:39.033762Z","iopub.execute_input":"2025-08-02T07:17:39.034014Z","iopub.status.idle":"2025-08-02T07:18:07.083900Z","shell.execute_reply.started":"2025-08-02T07:17:39.033993Z","shell.execute_reply":"2025-08-02T07:18:07.083043Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-08-02 07:17:41.648341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754119061.860074      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754119061.920680      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🔹 Train Rows: 229418\n🔹 Test Rows:  98323\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"#### inference","metadata":{}},{"cell_type":"code","source":"# Load test.csv for inference (no TARGET column)\ntest_df = pd.read_csv('/kaggle/input/prediction/HACKATHON_PREDICTION_DATA.CSV.xls')\nX_test_inference = test_df.drop(columns=['UNIQUE_ID']).copy()\ntest_unique_ids = test_df['UNIQUE_ID'].copy()\n\n# Handle placeholders and encode categoricals\nX_test_inference.replace([\"\\\\N\", \"NA\", \"NaN\", \"null\", \"\"], np.nan, inplace=True)\nfor col in X_test_inference.select_dtypes(include=['object', 'category']).columns:\n    X_test_inference[col] = X_test_inference[col].astype(str)\n    X_test_inference[col] = LabelEncoder().fit_transform(X_test_inference[col])\n\n# Impute missing values\nX_test_inference = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(X_test_inference), columns=X_test_inference.columns)\n\n# Scale features using the same scaler fitted on training data\nX_test_scaled = scaler.transform(X_test_inference)  # Use transform, not fit_transform\nprint(f\"🔹 Test inference rows: {len(X_test_scaled)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T07:18:07.084783Z","iopub.execute_input":"2025-08-02T07:18:07.085279Z","iopub.status.idle":"2025-08-02T07:18:17.343864Z","shell.execute_reply.started":"2025-08-02T07:18:07.085260Z","shell.execute_reply":"2025-08-02T07:18:17.342968Z"}},"outputs":[{"name":"stdout","text":"🔹 Test inference rows: 191693\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.metrics import recall_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Assume X, y already preprocessed and split:\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.3, random_state=42)\n\n# Calculate scale_pos_weight: (negatives / positives)\nneg, pos = np.bincount(y_train)\nscale_pos_weight = neg / pos\n\n# Initialize XGBoost classifier with scale_pos_weight to prioritize recall\nmodel = xgb.XGBClassifier(\n    use_label_encoder=False,\n    eval_metric='logloss',\n    scale_pos_weight=scale_pos_weight,\n    max_depth=6,\n    learning_rate=0.1,\n    n_estimators=100,\n    objective='binary:logistic',\n    subsample=0.8,\n    colsample_bytree=0.8,\n    seed=42\n)\n\n# Train model\nmodel.fit(X_train, y_train)\n\n# Predict probabilities\ny_probs = model.predict_proba(X_test)[:, 1]\n\n# Tune threshold to maximize recall\nthreshold = 0.3  # Lower threshold to increase recall\ny_pred = (y_probs > threshold).astype(int)\n\n# Evaluation\nprint(f\"Recall at threshold {threshold}: {recall_score(y_test, y_pred):.4f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\nprint(f\"TP: {tp}, FP: {fp}, TN: {tn}, FN: {fn}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:18:17.346275Z","iopub.execute_input":"2025-08-02T07:18:17.346791Z","iopub.status.idle":"2025-08-02T07:18:28.352286Z","shell.execute_reply.started":"2025-08-02T07:18:17.346771Z","shell.execute_reply":"2025-08-02T07:18:28.351102Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Recall at threshold 0.3: 0.9316\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.99      0.70      0.82     87691\n           1       0.28      0.93      0.43     10632\n\n    accuracy                           0.73     98323\n   macro avg       0.63      0.82      0.62     98323\nweighted avg       0.91      0.73      0.78     98323\n\nTP: 9905, FP: 25932, TN: 61759, FN: 727\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"#### inference","metadata":{}},{"cell_type":"code","source":"# Stage 1: XGBoost predictions with threshold 0.3\ntest_probs_stage1 = model.predict_proba(X_test_scaled)[:, 1]\ntest_pred_stage1 = (test_probs_stage1 > 0.3).astype(int)\n\nprint(f\"Stage 1 Results:\")\nprint(f\"Total test samples: {len(test_pred_stage1)}\")\nprint(f\"Stage 1 positive predictions: {test_pred_stage1.sum()}\")\nprint(f\"Stage 1 negative predictions: {len(test_pred_stage1) - test_pred_stage1.sum()}\")\nprint(f\"Samples proceeding to Stage 2: {test_pred_stage1.sum()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T07:18:28.353138Z","iopub.execute_input":"2025-08-02T07:18:28.353589Z","iopub.status.idle":"2025-08-02T07:18:28.704811Z","shell.execute_reply.started":"2025-08-02T07:18:28.353561Z","shell.execute_reply":"2025-08-02T07:18:28.703930Z"}},"outputs":[{"name":"stdout","text":"Stage 1 Results:\nTotal test samples: 191693\nStage 1 positive predictions: 72272\nStage 1 negative predictions: 119421\nSamples proceeding to Stage 2: 72272\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Attach predictions and probabilities to the original test set\noriginal_test_df = df.loc[y_test.index].copy()\noriginal_test_df['y_pred'] = y_pred\noriginal_test_df['y_prob'] = y_probs\n\n\n# ✅ df1: All predicted frauds (y_pred == 1)\ndf1 = original_test_df[original_test_df['y_pred'] == 1].copy()\ndf1_p1 = df1['y_prob'].values\n\n# ✅ df2: All predicted non-frauds (y_pred == 0)\ndf2 = original_test_df[original_test_df['y_pred'] == 0].copy()\ndf2_p1 = df2['y_prob'].values\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:18:28.705775Z","iopub.execute_input":"2025-08-02T07:18:28.706132Z","iopub.status.idle":"2025-08-02T07:18:28.940146Z","shell.execute_reply.started":"2025-08-02T07:18:28.706101Z","shell.execute_reply":"2025-08-02T07:18:28.939157Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Prepare data for Stage 2: only Stage 1 positives\nstage1_positive_indices = np.where(test_pred_stage1 == 1)[0]\n\nif len(stage1_positive_indices) > 0:\n    # Create dataframe for Stage 2 processing\n    test_stage2_df = test_df.iloc[stage1_positive_indices].copy()\n    test_stage2_df['stage1_pred'] = 1\n    test_stage2_df['stage1_prob'] = test_probs_stage1[stage1_positive_indices]\n    \n    print(f\"Stage 2 processing {len(test_stage2_df)} samples that were positive in Stage 1\")\nelse:\n    print(\"No samples were positive in Stage 1 - all predictions will be 0\")\n    test_stage2_df = pd.DataFrame()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T07:18:28.941258Z","iopub.execute_input":"2025-08-02T07:18:28.942085Z","iopub.status.idle":"2025-08-02T07:18:29.007521Z","shell.execute_reply.started":"2025-08-02T07:18:28.942051Z","shell.execute_reply":"2025-08-02T07:18:29.006481Z"}},"outputs":[{"name":"stdout","text":"Stage 2 processing 72272 samples that were positive in Stage 1\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n\n# Step 1: Split features and target\nX = df1.drop(columns=['TARGET', 'UNIQUE_ID']).copy()\ny = df1['TARGET']\n\n# Step 2: Handle missing values and encode categoricals\nX.replace([\"\\\\N\", \"NA\", \"NaN\", \"null\", \"\"], np.nan, inplace=True)\nfor col in X.select_dtypes(include=['object', 'category']).columns:\n    X[col] = X[col].astype(str)\n    X[col] = LabelEncoder().fit_transform(X[col])\n\n# Step 3: Impute and scale\nX = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(X), columns=X.columns)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Step 4: Train/test split (for model training)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, test_size=0.3, random_state=42)\n\n# Step 5: Define models\nmodels = [\n    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n    ('LightGBM', LGBMClassifier()),\n    ('CatBoost', CatBoostClassifier(verbose=0)),\n    ('ExtraTrees', ExtraTreesClassifier()),\n    ('MLP', MLPClassifier(max_iter=300)),\n    ('LogisticRegression', LogisticRegression(max_iter=500)),\n    ('RandomForest', RandomForestClassifier())\n]\n\n# Step 6: Fit models and predict on full X_scaled\noutput_df = pd.DataFrame()\nfor i, (name, model) in enumerate(models, 1):\n    print(f\"⏳ Training {name}...\")\n    model.fit(X_train, y_train)\n    probs = model.predict_proba(X_scaled)[:, 1]  # Probabilities on full Stage 2 data\n    output_df[f'model_{i}_prob'] = probs\n\n# Add true labels\noutput_df['TARGET'] = y.values\n\n# Step 7: Save output\noutput_df.to_csv(\"stage2_prediction_matrix_with_target.csv\", index=False)\nprint(\"✅ Saved stage2_prediction_matrix_with_target.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:18:29.008541Z","iopub.execute_input":"2025-08-02T07:18:29.008828Z","iopub.status.idle":"2025-08-02T07:20:25.020176Z","shell.execute_reply.started":"2025-08-02T07:18:29.008804Z","shell.execute_reply":"2025-08-02T07:20:25.019239Z"},"trusted":true},"outputs":[{"name":"stdout","text":"⏳ Training XGBoost...\n⏳ Training LightGBM...\n[LightGBM] [Info] Number of positive: 6933, number of negative: 18152\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020616 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 26895\n[LightGBM] [Info] Number of data points in the train set: 25085, number of used features: 138\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.276380 -> initscore=-0.962488\n[LightGBM] [Info] Start training from score -0.962488\n⏳ Training CatBoost...\n⏳ Training ExtraTrees...\n⏳ Training MLP...\n⏳ Training LogisticRegression...\n⏳ Training RandomForest...\n✅ Saved stage2_prediction_matrix_with_target.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### inference","metadata":{}},{"cell_type":"code","source":"# Stage 2 processing: ensemble models for Stage 1 positives only\nif len(test_stage2_df) > 0:\n    X_test_stage2 = test_stage2_df.drop(columns=['UNIQUE_ID']).copy()\n    \n    # Handle placeholders and encode categoricals\n    X_test_stage2.replace([\"\\\\N\", \"NA\", \"NaN\", \"null\", \"\"], np.nan, inplace=True)\n    for col in X_test_stage2.select_dtypes(include=['object', 'category']).columns:\n        X_test_stage2[col] = X_test_stage2[col].astype(str)\n        X_test_stage2[col] = LabelEncoder().fit_transform(X_test_stage2[col])\n    \n    # Impute missing values\n    X_test_stage2 = pd.DataFrame(SimpleImputer(strategy='median').fit_transform(X_test_stage2), columns=X_test_stage2.columns)\n    \n    # Scale features\n    scaler_stage2 = StandardScaler()\n    X_test_stage2_scaled = scaler_stage2.fit_transform(X_test_stage2)\n    \n    # Generate ensemble predictions\n    test_ensemble_df = pd.DataFrame()\n    for i, (name, model_obj) in enumerate(models, 1):\n        probs = model_obj.predict_proba(X_test_stage2_scaled)[:, 1]\n        test_ensemble_df[f'model_{i}_prob'] = probs\n    \n    print(f\"Generated ensemble predictions for {len(test_ensemble_df)} Stage 2 samples\")\nelse:\n    test_ensemble_df = pd.DataFrame()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T07:42:40.153607Z","iopub.execute_input":"2025-08-02T07:42:40.153883Z","iopub.status.idle":"2025-08-02T07:42:47.899918Z","shell.execute_reply.started":"2025-08-02T07:42:40.153863Z","shell.execute_reply":"2025-08-02T07:42:47.898500Z"}},"outputs":[{"name":"stdout","text":"Generated ensemble predictions for 72272 Stage 2 samples\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, roc_auc_score\n\n# Load the prediction matrix\ndf = output_df\n\n# Feature matrix (7 model probs)\nX = df.drop(columns=['TARGET'])\ny = df['TARGET']\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:47:58.977828Z","iopub.execute_input":"2025-08-02T07:47:58.978190Z","iopub.status.idle":"2025-08-02T07:47:59.087160Z","shell.execute_reply.started":"2025-08-02T07:47:58.978170Z","shell.execute_reply":"2025-08-02T07:47:59.086140Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load prediction matrix\ndf = pd.read_csv(\"stage2_prediction_matrix_with_target.csv\")\n\n# Features and label\nX = df.drop(columns=['TARGET'])\ny = df['TARGET']\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\n# Train a model (you can replace this with any classifier)\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Predict probabilities\ny_probs = model.predict_proba(X_test)[:, 1]\n\n# === 🔥 TUNE THIS THRESHOLD ===\nthreshold = 0.25\n\n# Apply threshold\ny_pred = (y_probs >= threshold).astype(int)\n\n# Confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n\n# Print breakdown\nprint(\"=== Confusion Matrix Breakdown ===\")\nprint(f\"Actual 0 → Predicted 0: {tn}\")\nprint(f\"Actual 0 → Predicted 1: {fp}\")\nprint(f\"Actual 1 → Predicted 0: {fn}\")\nprint(f\"Actual 1 → Predicted 1: {tp}\")\nprint()\n\n# Metrics\nprecision = tp / (tp + fp) if (tp + fp) != 0 else 0\nrecall    = tp / (tp + fn) if (tp + fn) != 0 else 0\nf1_score  = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\nauc       = roc_auc_score(y_test, y_probs)\n\nprint(\"=== Metrics at Threshold =\", threshold, \"===\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall   : {recall:.4f}\")\nprint(f\"F1 Score : {f1_score:.4f}\")\nprint(f\"AUC      : {auc:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2025-08-02T07:48:25.941263Z","iopub.execute_input":"2025-08-02T07:48:25.941940Z","iopub.status.idle":"2025-08-02T07:48:26.204080Z","shell.execute_reply.started":"2025-08-02T07:48:25.941909Z","shell.execute_reply":"2025-08-02T07:48:26.203131Z"},"trusted":true},"outputs":[{"name":"stdout","text":"=== Confusion Matrix Breakdown ===\nActual 0 → Predicted 0: 4146\nActual 0 → Predicted 1: 1041\nActual 1 → Predicted 0: 720\nActual 1 → Predicted 1: 1261\n\n=== Metrics at Threshold = 0.25 ===\nPrecision: 0.5478\nRecall   : 0.6365\nF1 Score : 0.5888\nAUC      : 0.7919\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"#### inference","metadata":{}},{"cell_type":"code","source":"# Stage 2 final predictions using trained LogisticRegression model\nif len(test_ensemble_df) > 0:\n    test_probs_stage2 = model.predict_proba(test_ensemble_df)[:, 1]  # Use trained LogisticRegression\n    test_pred_stage2 = (test_probs_stage2 >= 0.25).astype(int)  # Threshold 0.05\n    \n    print(f\"Stage 2 Results:\")\n    print(f\"Stage 2 positive predictions: {test_pred_stage2.sum()}\")\n    print(f\"Stage 2 negative predictions: {len(test_pred_stage2) - test_pred_stage2.sum()}\")\nelse:\n    test_pred_stage2 = np.array([])\n    print(\"No Stage 2 predictions needed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T07:48:56.908538Z","iopub.execute_input":"2025-08-02T07:48:56.909124Z","iopub.status.idle":"2025-08-02T07:48:56.920138Z","shell.execute_reply.started":"2025-08-02T07:48:56.909097Z","shell.execute_reply":"2025-08-02T07:48:56.919128Z"}},"outputs":[{"name":"stdout","text":"Stage 2 Results:\nStage 2 positive predictions: 21092\nStage 2 negative predictions: 51180\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# INFERENCE","metadata":{}},{"cell_type":"code","source":"# Final Two-Stage Predictions for Test Dataset\n# Stage 1: All samples → XGBoost with threshold 0.3\n# Stage 2: Only Stage 1 positives → Ensemble + LogisticRegression with threshold 0.05\n\n# Initialize all predictions as 0\nfinal_test_predictions = np.zeros(len(test_df), dtype=int)\n\nprint(f\"=== Two-Stage Prediction Pipeline ===\")\nprint(f\"Total test samples: {len(test_df)}\")\nprint(f\"Stage 1 positives: {test_pred_stage1.sum()}\")\n\n# Only Stage 1 positives that also pass Stage 2 become final positives\nif len(test_pred_stage2) > 0:\n    final_test_predictions[stage1_positive_indices] = test_pred_stage2\n    final_positive_count = final_test_predictions.sum()\n    \n    print(f\"Stage 2 positives: {test_pred_stage2.sum()}\")\n    print(f\"Final positives: {final_positive_count}\")\nelse:\n    final_positive_count = 0\n    print(f\"Stage 2 positives: 0\")\n    print(f\"Final positives: 0\")\n\nprint(f\"Final negatives: {len(final_test_predictions) - final_positive_count}\")\nprint(f\"Final fraud detection rate: {final_positive_count/len(final_test_predictions)*100:.2f}%\")\n\n# Create final output CSV\nfinal_test_output = pd.DataFrame({\n    'UNIQUE_ID': test_unique_ids,\n    'FINAL_PREDICTION': final_test_predictions\n})\n\n# Save predictions\nfinal_test_output.to_csv('test_predictions.csv', index=False)\nprint(f\"\\n✅ Final test predictions saved to 'test_predictions.csv'\")\nprint(f\"Output format: UNIQUE_ID, FINAL_PREDICTION\")\nprint(f\"Shape: {final_test_output.shape}\")\n\n# Show sample predictions\nprint(f\"\\nFirst 10 predictions:\")\nprint(final_test_output.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T07:49:00.897724Z","iopub.execute_input":"2025-08-02T07:49:00.898045Z","iopub.status.idle":"2025-08-02T07:49:01.029509Z","shell.execute_reply.started":"2025-08-02T07:49:00.898026Z","shell.execute_reply":"2025-08-02T07:49:01.028426Z"}},"outputs":[{"name":"stdout","text":"=== Two-Stage Prediction Pipeline ===\nTotal test samples: 191693\nStage 1 positives: 72272\nStage 2 positives: 21092\nFinal positives: 21092\nFinal negatives: 170601\nFinal fraud detection rate: 11.00%\n\n✅ Final test predictions saved to 'test_predictions.csv'\nOutput format: UNIQUE_ID, FINAL_PREDICTION\nShape: (191693, 2)\n\nFirst 10 predictions:\n   UNIQUE_ID  FINAL_PREDICTION\n0       2202                 0\n1       2209                 0\n2       2211                 0\n3       2217                 0\n4       2218                 0\n5       4197                 0\n6       4198                 0\n7       4200                 0\n8       4202                 0\n9       4203                 0\n","output_type":"stream"}],"execution_count":18}]}